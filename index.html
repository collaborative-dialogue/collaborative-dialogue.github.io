<!DOCTYPE html>
<html lang="en">

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>
<link href='https://fonts.googleapis.com/css?family=Asap' rel='stylesheet'>


<style type="text/css">
body {
    font-family: "Asap", "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 36px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:0.6cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.col-8{
width: 12.5%;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 0px;
    margin-bottom: 20px;
	text-align: left;
}

.caption-up {
    font-size: 16px;
    color: #666;
    margin-top: -8px;
    margin-left: 50px;
    margin-bottom: 20px;
	text-align: left;
}

.caption-right {
    font-size: 16px;
    color: #666;
    margin-top: 0px;
    margin-left: 0px;
    margin-bottom: 30px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}



.image-container {
    text-align: center;
}

.image-container img {
    border: 2px solid black;
    width: 100%;
}

.image-container img:hover {
    opacity: 0.7;
}

.image-container .image-caption {
    text-align: center;
}

.data-collection-image {
    width: 33%;
    display: inline-block;
    padding: 15px;
}

.result-image-container {
    width: 100%;
    text-align: center;
}

.result-image {
    width: 80%;
    display: inline-block;
}


/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">
<link rel="stylesheet" href="simplegrid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Decision-Oriented Dialogue for Human-AI Collaboration</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Decision-Oriented Dialogue for Human-AI Collaboration"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@nickatomlin">
        <meta name="twitter:title" content="Decision-Oriented Dialogue for Human-AI Collaboration">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

<body>

<div class="container">
    <div class="paper-title">
    <h1> 
        Decision-Oriented Dialogue for Human-AI Collaboration
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://jessylin.com">Jessy Lin<sup>*1</sup></a>,
                <a href="https://people.eecs.berkeley.edu/~nicholas_tomlin/">Nicholas Tomlin<sup>*1</sup></a>,
                <a href="https://www.mit.edu/~jda/">Jacob Andreas<sup>23</sup></a>,
                <a href="https://www.cs.jhu.edu/~jason/">Jason Eisner<sup>24</sup></a>,
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> UC Berkeley</span>
            <span><sup>2</sup> Microsoft Semantic Machines</span>
            <span><sup>3</sup> MIT</span>
            <span><sup>4</sup> Johns Hopkins</span><br/>
        </div>

        <!---
        <div class="affil-row">
            <div class="venue text-center"><b>arXiv preprint</b></div>
        </div>
        -->

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/pdf/2305.20076.pdf">
                <span class="material-icons"> description </span> <br/>
                 Paper
            </a>
            <a class="paper-btn" href="https://github.com/jlin816/dialop">
                <span class="material-icons"> code </span><br/>
                Code
            </a>
            </div>
        </div>
    </div>

    <!-- <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="50%" controls autoplay loop muted playsinline class="video-background " >
                    <source src="img/debate.m4v" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section> -->

    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                We describe a class of tasks called <i>decision-oriented dialogues</i>, in which AI
                assistants must collaborate with one or more humans via natural language to
                help them make complex decisions. We formalize three domains in which users
                face everyday decisions: (1) choosing an assignment of reviewers to conference
                papers, (2) planning a multi-step itinerary in a city, and (3) negotiating
                travel plans for a group of friends. In each of these settings, AI assistants
                and users have disparate abilities that they must combine to arrive at the best
                decision: assistants can access and process large amounts of information, while
                users have preferences and constraints external to the system. For each task,
                we build a dialogue environment where agents receive a reward based on the
                quality of the final decision they reach. Using these environments, we collect
                human-human dialogues with humans playing the role of assistant. To compare how
                current AI assistants communicate in these settings, we present baselines using
                large language models in self-play. Finally, we highlight a number of
                challenges models face in decision-oriented dialogues, ranging from efficient
                communication to reasoning and optimization, and release our environments as a
                testbed for future modeling work.
            </p>
        </div>
    </section>

    <section id="defn"/>
        <hr>
        <h2>Definition</h2>
        <div class="flex-row">
            <p>
                We define <i>decision-oriented dialogue</i> as a class of tasks in which users and assistants must collaborate to make structured decisions. This may be viewed as an extension of task-oriented dialogue and is also a subclass of mixed-initiative dialogue.
                In a decision-oriented dialogue, there is an objective measure of utility based on the final decision a user reaches. This allows us to automatically evaluate the quality of AI assistants in these settings.
            </p>
        </div>
    </section>

    <section id="tasks"/>
        <hr>
        <h2>Tasks & Data</h2>
        <div class="flex-row">
            <p>
                We designed three decision-oriented dialogue tasks, along with corresponding interfaces for data collection. (1) In Optimization, two agents take on the role of conference area chairs, assigning reviewers to conference papers when each agent has only has partial information about reviewer-paper affinities. (2) In Planning, an assistant with knowledge of a city must assist a human with building an itinerary based on their preferences. (3) In Mediation, multiple users must collaborate with an assistant in order to resolve group scheduling challenges.
            </p>
            <p>We collected human-human dialogues for each of these tasks using the interfaces below:</p>
            <div class="image-container">
                <div class="data-collection-image">
                    <a href="http://reviewer-matching.herokuapp.com" target="_blank">
                        <img src="img/matching.png" alt="Matching">
                    </a>
                    <div class="image-caption">Optimization</div>
                </div>

                <div class="data-collection-image">
                    <a href="https://itinerary-planning.herokuapp.com" target="_blank">
                        <img src="img/planning_agent.png" alt="Planning Agent">
                    </a>
                    <div class="image-caption">Planning</div>
                </div>
                
                <div class="data-collection-image">
                    <a href="https://collaborative-dialogue.herokuapp.com" target="_blank">
                        <img src="img/mediation_agent.png" alt="Mediation Agent">
                    </a>
                    <div class="image-caption">Mediation</div>
                </div>
            </div>
            <p>In order to view an environment, click on the corresponding link above multiple times. You will need to open as many browser windows as there are players in the game, which is two for Optimization and Planning, and three for Mediation.</p>
            <p>In total, we collected 409 human-human dialogues consisting of a total 5253 messages and 58K words. Data is available <a href="https://github.com/jlin816/dialop">here</a>.</p>
        </div>
    </section>

    <section id="models"/>
        <hr>
        <h2>Models & Environments</h2>
        <div class="flex-row">
            <p>
                We also release environments which allow language models to access the same information that humans can see in the web interfaces above. In the case of Planning, these environments include additional tools which allow language models to query for external information, because the information cannot be linearized into the context length of most models (e.g., 4096 tokens). We then run GPT-3 (text-davinci-003) on all three tasks in self-play and compare its performance to that of humans.
            </p>
        </div>
    </section>

    <section id="results"/>
        <hr>
        <h2>Results</h2>
        We find that GPT-3 (in self-play) has longer dialogues than humans while achieving lower scores on all of our tasks:
        <div class="flex-row">
            <div class="result-image-container">
                <img class="result-image" src="img/selfplay-len.png" alt="Self-play Results"></div>
            </div>
            <p>
                We also include results for a new evaluation technique called <i>prompted self-play</i> (PSP). In this condition, we prompt models with 50% or 75% of corresponding human-human dialogues and let models continue from these prefixes in self-play. We also consider a <i>proposal</i> condition in which models have access to all messages in a human-human dialogue except the final proposal and must generate the final proposal themselves. Results are shown below:
            </p>
            <div class="result-image-container">
                <img class="result-image" src="img/psp.png" alt="Prompted Self-play Results"></div>
            </div>
            <p>
                Model performance is different across all three tasks. Models fail to outperform a random baseline on the Optimization task. In Planning, models perform better the longer the human prefix, suggesting that they are capable of conditioning on human messages. In Mediation, models perform better than a random baseline but still worse than humans, but do not show much variation across the different PSP conditions.
                These results suggest that models have yet to close the gap to human performance in communicating efficiently to collaborate on good solutions
            </p>
        </div>
    </section>


    <!-- <div class="section">
        <h2>Results</h2>
        <hr>
        <p>
            Existing approaches towards improving language models have primarily focused on improving the performance of a
            single language generator. We illustrate how we may treat different instances of the same language models as a
            "multiagent society", where individual language model generate and critique the language generations of other instances of the language model.  We find that the final answer generated after such a procedure is both more factually accurate and solves reasoning questions more accurately.  We illustrate below the quantitative difference between multiagent debate and single agent generation on  different domains in reasoning and factual validity.
        </p>
        <figure>
            <a>
                <img class="centered" width="100%" src="img/accuracy_small.png"> 
            </a>
            <p class="caption-up">
                <b>Multiagent Debate Improves Reasoning and Factual Accuracy.</b> Accuracy of traditional inference and our multi-agent debate over six benchmarks (chess move optimality reported as a normalized score) <br>
        </figure>
    </div>

    <div class="section">
        <h2>Performance with More Agents and Rounds of Debate</h2>
        <hr>
        <p>
           Throughout the experiments in the paper, we used a total of 3 language models agents which debate for a total of two arounds, due to computational cost. However, the underlying performance of multiagent debate can be substantially improved by either using more agents or by having more of rounds of debate. Below, we report the accuracy of solving arithmetic expressions as these individual factors are varied.
        </p>
        <figure>
            <a>
                <img class="centered" width="95%" src="img/agent_round.png"> 
            </a>
            <p class="caption-right">
                <b>(a) Performance with Increased Agents.</b> Arithmetic performance improves as the number of underlying agents involved in debate increases. <b>(b) Performance with Increased Rounds.</b> Arithmetic performance improves as the number of rounds of underlying debate increases. <br>
        </figure>
    </div>

    <div class="section">
        <h2>Debate Between chatGPT and Bard</h2>
        <hr>
        <p>
           While we mainly study multiagent debate across multiple instances of the same language model, multiagent debate
           can also be used to combine different lanuage models together. This enables the strengths of one model to enable
           better performance in another. Below, we illustrate how a combination of both ChatGPT and Bard can be used together
           to solve a difficult Grade School Math problem.
        </p>
    </div>
        <figure>
            <a>
                <center>
                <img class="centered" width="90%" src="img/gpt_bard.png"> 
                </center>
            </a>
            <p class="caption">
                <b>Debate Between ChatGPT and Bard.</b> Illustration of debate between different models. While both models generate incorrect responses to the initial GSM8K problem, debate between the models enables them to generate the correct final answer.<br>
        </figure>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <p>
        </p>
        <div>
            <div class="list-group">
                <a href=""
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

        <div class="section">
            <h2>Our Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on combining and composing different models! A full list can be found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!
            </p>


            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/compose_pretrain.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://energy-based-model.github.io/composing-pretrained-models/">Composing Pretrained Models through Iterative Consensus</a>

                    </div>
                    <div>
                        We present a method to combine different large pretrained models together by having individual models communicate with each other through iterative consensus. We illustrate how this combination of models can do zero-shot VQA, image generation, reasoning, and image generation.
                    </div>
                </div>
            </div>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <video width="100%" playsinline="" autoplay="" preload="" muted="">
                    <source src="img/recyle.m4v" type="video/mp4">
                </video>
            </div>
        </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://energy-based-model.github.io/reduce-reuse-recycle/">Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</a>
            </div>
            <div>
                We present a method to combine different diffusion models with different probabilistic operations. To enable this, we propose new samplers, inspired by MCMC, to enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers.
            </div>
            </div>
            </div>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="img/teaser_glide.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional Visual Generation with Composable Diffusion Models</a>
        </div>
        <div>
            We present a method to improve the compositional generalization of diffusion models by composing
            different diffusion models together, drawing on the close connection of
            diffusion models with EBMs. We illustrate how compositional operators enable
            the ability to composing multiple sets of objects together as well as generate images subject to 
            complex text prompts.
        </div>
        </div>
        </div>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="img/comp_cartoon.png" class="img-fluid" alt="comp_carton" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/compositional-generation-inference/">Compositional Visual Generation with Energy Based Models</a>
        </div>
        <div>
            We present a set of compositional operators that enable EBMs to exhibit <b>zero-shot compositional</b> visual generation, enabling us to compose visual concepts
            (through operators of conjunction, disjunction, or negation) together in a zero-shot manner.
            Our approach enables us to generate faces given a  description
            ((Smiling AND Female) OR (NOT Smiling AND Male)) or to combine several different objects together.
        </div>
        </div>
        </div>



    <hr> -->

    <section>
        <br/>
        This webpage template was recycled from <a href='https://composable-models.github.io/llm_debate/'>here</a>.
    </section>

</div>

</body>
</html>
